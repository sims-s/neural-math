data:
  base: 30
  oos_path: data/oos_data_2^16.npy
  test_path: data/test_data_2^16.npy
  train_path: data/train_data_2^16.npy
io:
  checkpoint_every: 10000
  evaluate_every: 1
  evaluate_final: true
  save_path: ./models/factorization/2^16/epochs_400/
loader:
  oos:
    batch_size: 256
    shuffle: false
  test:
    batch_size: 256
    shuffle: false
  train:
    batch_size: 256
    random_sampling: true
metrics:
  max_num: -1
  n_beams: 2
  save_suffix: ''
  temperature: 1.0
model_args:
  attn_weight_xavier_init_constant: 0.5
  dim_feedforward: 512
  dropout: 0.05
  embed_dim: 128
  embedding_initialization: xavier
  extra_positional_encoding_relative_decoder_mha: true
  learn_positional_encoding: false
  max_decode_size: 64
  norm_first: false
  num_decoder_layers: 6
  num_encoder_layers: 6
  num_heads: 8
  positional_encoding_query_key_only: false
  positional_encoding_type: relative-transfxl
  repeat_positional_encoding: false
  scale_embeddings: false
  scale_embeddings_at_init: false
  shared_embeddings: true
optimizer:
  gradient_accumulation_steps: 1
  max_grad_norm: 1
  opt_args:
    lr: 0.001
    weight_decay: 0.1
  type: AdamW
problem_type: factorization
resume_training: false
scheduler:
  max_steps: -1
  nb_epochs: 400
  scheduler_args:
    nb_steps: 92400
    num_warmup_steps: 6000
  type: get_linear_schedule_with_warmup
tokenizer:
  n_tokens: 34
  pad_token_id: 31
verbose: true
wandb:
  enabled: false
  entity: sims-s
  project: integer-factorization
  watch_args:
    log: all
    log_freq: 50
